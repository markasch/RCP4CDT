# Introduction {.unnumbered}

A wide-scale, cross-facility workflow is a complex beast. It reunites users, jobs, and facilities, each with its resources and constraints. In the workflows that interest us, the facilites include data centres, HPC^[High Performance Computing] centres, and the network connections between these.

The basic problem can be resumed as follows: find an optimal schedule $S$ for a collection of jobs $J$ to be executed on a set of facilities $F,$ subject to constraints on resources, availability, precedences. The optimization can be performed for various objectives, or combinations of these, such as cost, project duration, facility availability, environmental impact. The presence of _uncertainty_ plays a central role and is included in the optimization process.

## The Exa-AtoW Project

This work is part of the Exa-AtoW project, a member of the NumPEx consortium.[ [https://numpex.org](https://numpex.org/) ]{.column-margin} The Exa-AToW project aims at providing solutions for the efficient management of large-scale workflows composed of HPDA, AI, and HPC tasks that are distributed over a continuum of resources ranging from the Exascale, HPC, and Data infrastructures.

Exa-AToW focuses on effective end-to-end solutions, at scale, by considering not only functional dimensions such as workflows and data logistics but also resource federation governance, cybersecurity, energy, and sustainability.

## Continuum Digital Twin or Shadow

Given the inherent complexity of Exascale workflows, they will inevitably be executed on a cross-facility infrastructure. Such a multi-component basis will inevtibaly be subject to uncertainties in availability, maintenance, cost, etc. In addition, the cybersecurity constraints will impose strict access conditions that make workflow testing basically impossible. And, more recently, the issue of sustainability and energy consumption by cyberinfrastructure [@IEA2025] is a critical issue, in particular for so-called hyperscalers and data centers used for AI training and inference. 

For all these reasons, the presence of a digital twin, or shadow, is indispensable for testing and planning workflow executions _before_ actually executing them. The computer science setup, based on micro-services and ontologies, is fully described in [@MGG2025implementation] and extended use-cases are presented in [@MGG2025usecases].


::: {#fig-modules}
```{mermaid}
graph LR
    A[SIM] --> B[SCD]
    B --> C[OPT]
    C --> A
```
Three modules of the CDT: simuulation (SIM), scheduling (SCD), optimization (OPT). An initial workflow definition enters at the left and an optimal workflow is produced.
:::

The core of the CDT is a set of
three modules: simulation, scheduling and optimization, as shown in @fig-modules. 
They can be used independently, or be chained together. These three are fed by
a shared database, based on a common ontology that contains descriptions of all the jobs to perform,
resources available, constraints to be respected, and any objectives
to be attained. This database is connected to, and communicates with
the real world---see  @fig-global.
This communication can take the form of a MADPP (machine actionable
data project plan), a user-interface, or a combination of the two [@MGG2025implementation].

::: {#fig-global}
![](graphics/CDT.png){width=80%}

Global software architecture of the CDT, consisting of three modules:
simulation (SIM), scheduling (SCD), optimization (OPT).
:::

All details can be found in [@MGG2025math].


## Uncertainty

 Deterministic schedules optimize for a world that, in theory, never materializes. Processing times vary, machines fail, networks are overloaded, demand shifts. As a result, a schedule that is "optimal" under perfect information often performs poorly when reality intervenes.  Stochastic formulations, on the other hand, explicitly hedge against variability. The resulting schedules may sacrifice some of the theoretical efficiency, but maintain feasibility when _disruptions_ occur, thus avoiding costly rescheduling or missed deadlines. Rather than a single makespan or cost figure, one can obtain distributions: "We meet the deadline with 95% confidence" which is far more informative than "the expected completion is Tuesday."

Hence, when uncertainty in job durations or data transfer arrivals is modeled, the true value of buffer capacity, parallel facilities, or overtime flexibility becomes visible. Deterministic models systematically undervalue these. We can compute the value of the stochastic solution (VSS), which quantifies what can be gained by solving the full stochastic program rather than just optimizing against average conditions. Note that averaging yields a deterministic problem.

The Value of Stochastic Solution (VSS) is defined as the difference between the Expectation of the Expected Value Solution (EEVS) and the optimal objective value of the Recourse Problem (RP). The VSS quantifies the benefit of using a stochastic model in place of a deterministic one in decision-making problems under uncertainty. A large VSS signals that the system is sensitive to variability, hedging decisions matter, and the deterministic approximation is potentially dangerous.

::: {.column-margin}
$$\mathrm{VSS} = \mathrm{EEV} - \mathrm{RP}$$
:::

In conclusion, when faced with expensive (financially and environmentally), time-consuming Exascale workflows, the need to consider uncertainty in the scheduling program is vital. Stochastic-based scheduling can take into account any uncertain resources and all uncertain costs---a good example would be variable energy costs. The solution thus obtained will permit a _hedging_ strategy. We can, in addition, perform risk analysis, where we compare alternative deployments of the workflow as a function of our _risk profile_.



